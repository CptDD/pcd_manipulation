 Horizon = 3
Parsing terminal tag
Parsing transitions
Parsing reward function
Done
Model = N6despot6POMDPXE
Random root seed = 152544000
Search depth = 3
Discount = 0.95
Simulation steps = 3
Number of scenarios = 500
Search time per step = 1
Regularization constant = 0
Lower bound = DEFAULT
Upper bound = DEFAULT
Policy simulation depth = 90
Target gap ratio = 0.95

####################################### Round 0 #######################################
Initial state: 
[robot_1:side, state_1:livarno]

pdf for 4096 particles:
 [robot_1:side, state_1:livarno] = 1

-----------------------------------Round 0 Step 0-----------------------------------
- Action = 0:atop
- State:
[robot_1:top, state_1:standard]
- Observation = 
[obs_sensor:ostandard]
- ObsProb = 0.5
- Reward = -15
- Current rewards:
  discounted / undiscounted = -15 / -15

-----------------------------------Round 0 Step 1-----------------------------------
- Action = 0:atop
- State:
[robot_1:top, state_1:livarno]
- Observation = 
[obs_sensor:olivarno]
- ObsProb = 0.25
- Reward = -15
- Current rewards:
  discounted / undiscounted = -29.25 / -30

-----------------------------------Round 0 Step 2-----------------------------------
- Action = 1:aside
- State:
[robot_1:side, state_1:standard]
- Observation = 
[obs_sensor:olivarno]
- ObsProb = 0.6
- Reward = -15
- Current rewards:
  discounted / undiscounted = -42.7875 / -45

Simulation terminated in 3 steps
Total discounted reward = -42.7875
Total undiscounted reward = -45

Completed 1 run(s).
Average total discounted reward (stderr) = -42.7875 (0)
Average total undiscounted reward (stderr) = -45 (0)
Total time: Real / CPU = 0.067461 / 0.06744s
